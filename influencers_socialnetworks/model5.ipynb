{
 "metadata": {
  "name": "model5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "# SAMPLE SUBMISSION TO THE BIG DATA HACKATHON 13-14 April 2013 'Influencers in a Social Network'\n",
      "# .... more info on Kaggle and links to go here\n",
      "#\n",
      "# written by Ferenc Husz\u00e1r, PeerIndex\n",
      "\n",
      "from sklearn import linear_model\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import svm\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn import preprocessing\n",
      "from sklearn import cross_validation\n",
      "from sklearn.metrics import auc_score\n",
      "import numpy as np\n",
      "\n",
      "###########################\n",
      "# LOADING TRAINING DATA\n",
      "###########################\n",
      "\n",
      "trainfile = open('train1.csv')\n",
      "header = trainfile.next().rstrip().split(',')\n",
      "\n",
      "y_train = []\n",
      "X_train_A = []\n",
      "X_train_B = []\n",
      "\n",
      "for line in trainfile:\n",
      "    splitted = line.rstrip().split(',')\n",
      "    label = int(splitted[0])\n",
      "    A_features = [float(item) for item in splitted[1:13]]\n",
      "    B_features = [float(item) for item in splitted[13:]]\n",
      "    y_train.append(label)\n",
      "    X_train_A.append(A_features)\n",
      "    X_train_B.append(B_features)\n",
      "trainfile.close()\n",
      "\n",
      "y_train = np.array(y_train)\n",
      "X_train_A = np.array(X_train_A)\n",
      "X_train_B = np.array(X_train_B)\n",
      "\n",
      "###########################\n",
      "# EXAMPLE BASELINE SOLUTION USING SCIKIT-LEARN\n",
      "#\n",
      "# using scikit-learn LogisticRegression module without fitting intercept\n",
      "# to make it more interesting instead of using the raw features we transform them logarithmically\n",
      "# the input to the classifier will be the difference between transformed features of A and B\n",
      "# the method roughly follows this procedure, except that we already start with pairwise data\n",
      "# http://fseoane.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/\n",
      "###########################\n",
      "\n",
      "def transform_features(x):\n",
      "    #return np.log(1+x)\n",
      "    #return preprocessing.MinMaxScaler().fit_transform(np.log(1+x))\n",
      "    #return np.log(1+preprocessing.MinMaxScaler().fit_transform(x))\n",
      "    return x\n",
      "\n",
      "X = transform_features(X_train_A) - transform_features(X_train_B)\n",
      "X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(X, y_train, test_size=0.2, random_state=0)\n",
      "#model = linear_model.LogisticRegression(fit_intercept=False)\n",
      "#model = RandomForestClassifier(n_estimators=150, min_samples_split=2, n_jobs=-1)\n",
      "#model = svm.SVC()\n",
      "model = ExtraTreesClassifier(random_state=0,compute_importances=True)\n",
      "#model.fit(X_train,y_train)\n",
      "X_train = model.fit(X_train,y_train).transform(X_train)\n",
      "# compute AuC score on the training data (BTW this is kind of useless due to overfitting, but hey, this is only an example solution)\n",
      "'''\n",
      "p_cv = model.predict_proba(X_cv)\n",
      "p_cv = p_cv[:,1:2]\n",
      "print p_cv\n",
      "print 'AuC score on CV data:',auc_score(y_cv,p_cv.T)\n",
      "'''\n",
      "\n",
      "\n",
      "###########################\n",
      "# READING TEST DATA\n",
      "###########################\n",
      "\n",
      "testfile = open('test1.csv')\n",
      "#ignore the test header\n",
      "testfile.next()\n",
      "\n",
      "X_test_A = []\n",
      "X_test_B = []\n",
      "for line in testfile:\n",
      "    splitted = line.rstrip().split(',')\n",
      "    A_features = [float(item) for item in splitted[0:12]]\n",
      "    B_features = [float(item) for item in splitted[12:]]\n",
      "    X_test_A.append(A_features)\n",
      "    X_test_B.append(B_features)\n",
      "testfile.close()\n",
      "\n",
      "X_test_A = np.array(X_test_A)\n",
      "X_test_B = np.array(X_test_B)\n",
      "\n",
      "# transform features in the same way as for training to ensure consistency\n",
      "X_test = transform_features(X_test_A) - transform_features(X_test_B)\n",
      "# compute probabilistic predictions\n",
      "p_test = model.predict_proba(X_test)\n",
      "#only need the probability of the 1 class\n",
      "p_test = p_test[:,1:2]\n",
      "\n",
      "###########################\n",
      "# WRITING SUBMISSION FILE\n",
      "###########################\n",
      "predfile = open('predictions.csv','w+')\n",
      "\n",
      "print >>predfile,','.join(header)\n",
      "for line in np.concatenate((p_test,X_test_A,X_test_B),axis=1):\n",
      "    print >>predfile, ','.join([str(item) for item in line])\n",
      "\n",
      "predfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Importance weights not computed. Please set the compute_importances parameter before fit.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-427ec7c0e263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#model.fit(X_train,y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;31m# compute AuC score on the training data (BTW this is kind of useless due to overfitting, but hey, this is only an example solution)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m '''\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/feature_selection/selector_mixin.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, threshold)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimportances\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 raise ValueError(\"Importance weights not computed. Please set\"\n\u001b[0m\u001b[1;32m     45\u001b[0m                                  \u001b[0;34m\" the compute_importances parameter before \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                  \"fit.\")\n",
        "\u001b[0;31mValueError\u001b[0m: Importance weights not computed. Please set the compute_importances parameter before fit."
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}