{
 "metadata": {
  "name": "model6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "\n",
      "# SAMPLE SUBMISSION TO THE BIG DATA HACKATHON 13-14 April 2013 'Influencers in a Social Network'\n",
      "# .... more info on Kaggle and links to go here\n",
      "#\n",
      "# written by Ferenc Husz\u00e1r, PeerIndex\n",
      "\n",
      "from sklearn import linear_model\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import svm\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn import preprocessing\n",
      "from sklearn import cross_validation\n",
      "from sklearn.metrics import auc_score\n",
      "import numpy as np\n",
      "\n",
      "###########################\n",
      "# LOADING TRAINING DATA\n",
      "###########################\n",
      "\n",
      "trainfile = open('train.csv')\n",
      "header = trainfile.next().rstrip().split(',')\n",
      "\n",
      "y_train = []\n",
      "X_train_A = []\n",
      "X_train_B = []\n",
      "\n",
      "for line in trainfile:\n",
      "    splitted = line.rstrip().split(',')\n",
      "    label = int(splitted[0])\n",
      "    A_features = [float(item) for item in splitted[1:12]]\n",
      "    B_features = [float(item) for item in splitted[12:]]\n",
      "    y_train.append(label)\n",
      "    X_train_A.append(A_features)\n",
      "    X_train_B.append(B_features)\n",
      "trainfile.close()\n",
      "\n",
      "y_train = np.array(y_train)\n",
      "X_train_A = np.array(X_train_A)\n",
      "X_train_B = np.array(X_train_B)\n",
      "\n",
      "###########################\n",
      "# EXAMPLE BASELINE SOLUTION USING SCIKIT-LEARN\n",
      "#\n",
      "# using scikit-learn LogisticRegression module without fitting intercept\n",
      "# to make it more interesting instead of using the raw features we transform them logarithmically\n",
      "# the input to the classifier will be the difference between transformed features of A and B\n",
      "# the method roughly follows this procedure, except that we already start with pairwise data\n",
      "# http://fseoane.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/\n",
      "###########################\n",
      "\n",
      "def transform_features(x):\n",
      "    #return np.log(1+x)\n",
      "    #return preprocessing.MinMaxScaler().fit_transform(np.log(1+x))\n",
      "    #return np.log(1+preprocessing.MinMaxScaler().fit_transform(x))\n",
      "    return x\n",
      "\n",
      "X = transform_features(X_train_A) - transform_features(X_train_B)\n",
      "model = ExtraTreesClassifier(random_state=0,compute_importances=True)\n",
      "#model.fit(X_train,y_train)\n",
      "print X.shape\n",
      "#X = [float(item) for item in [X[0],X[2],X[5],X[7],X[9]]]\n",
      "\n",
      "X = model.fit(X,y_train).transform(X)\n",
      "print X.shape\n",
      "importances = model.feature_importances_\n",
      "indices = np.argsort(importances)[::-1]\n",
      "print indices\n",
      "print X\n",
      "#X = X_tmp[:,[0,2,5,7,9]]\n",
      "X_train, X_cv, y_train, y_cv = cross_validation.train_test_split(X, y_train, test_size=0.2, random_state=0)\n",
      "print X_train.shape\n",
      "#model = linear_model.LogisticRegression(fit_intercept=False)\n",
      "#model = RandomForestClassifier(n_estimators=150, min_samples_split=2, n_jobs=-1)\n",
      "#model = svm.SVC()\n",
      "# compute AuC score on the training data (BTW this is kind of useless due to overfitting, but hey, this is only an example solution)\n",
      "\n",
      "model = linear_model.LogisticRegression(fit_intercept=False)\n",
      "model.fit(X_train,y_train)\n",
      "p_cv = model.predict_proba(X_cv)\n",
      "p_cv = p_cv[:,1:2]\n",
      "print p_cv\n",
      "print 'AuC score on CV data:',auc_score(y_cv,p_cv.T)\n",
      "\n",
      "\n",
      "\n",
      "###########################\n",
      "# READING TEST DATA\n",
      "###########################\n",
      "\n",
      "testfile = open('test.csv')\n",
      "#ignore the test header\n",
      "testfile.next()\n",
      "\n",
      "\n",
      "X_test_A = []\n",
      "X_test_B = []\n",
      "for line in testfile:\n",
      "    splitted = line.rstrip().split(',')\n",
      "    #A_features = [float(item) for item in splitted[0:12]]\n",
      "    A_features = [float(item) for item in [splitted[0],splitted[2],splitted[5],splitted[7],splitted[9]]]\n",
      "    #B_features = [float(item) for item in splitted[12:]]\n",
      "    B_features = [float(item) for item in [splitted[12],splitted[14],splitted[17],splitted[19],splitted[21]]]\n",
      "    X_test_A.append(A_features)\n",
      "    X_test_B.append(B_features)\n",
      "testfile.close()\n",
      "\n",
      "X_test_A = np.array(X_test_A)\n",
      "X_test_B = np.array(X_test_B)\n",
      "\n",
      "# transform features in the same way as for training to ensure consistency\n",
      "X_test = transform_features(X_test_A) - transform_features(X_test_B)\n",
      "# compute probabilistic predictions\n",
      "p_test = model.predict_proba(X_test)\n",
      "#only need the probability of the 1 class\n",
      "p_test = p_test[:,1:2]\n",
      "\n",
      "###########################\n",
      "# WRITING SUBMISSION FILE\n",
      "###########################\n",
      "predfile = open('predictions.csv','w+')\n",
      "\n",
      "print >>predfile,','.join(header)\n",
      "for line in np.concatenate((p_test,X_test_A,X_test_B),axis=1):\n",
      "    print >>predfile, ','.join([str(item) for item in line])\n",
      "\n",
      "predfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(5500, 11)\n",
        "(5500, 5)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ 2  5  0  6  8  3  4 10  7  1  9]\n",
        "[[ -3.42350000e+04  -1.68600000e+03  -8.10382784e+00  -2.31919685e-01\n",
        "   -6.40000000e+01]\n",
        " [ -1.76710000e+04  -1.38200000e+03   2.48165175e+00   5.46816093e-01\n",
        "    2.06000000e+02]\n",
        " [  3.68800000e+03  -1.05000000e+02   4.75831697e+00   4.90702248e-01\n",
        "    9.20000000e+01]\n",
        " ..., \n",
        " [  7.83100000e+03   5.66000000e+02   1.81576212e-01  -7.77790227e-01\n",
        "    1.17000000e+02]\n",
        " [ -5.74240000e+04  -2.16810000e+04  -1.38824174e+00  -1.24457004e+00\n",
        "    1.90700000e+03]\n",
        " [ -5.03376000e+05  -1.27660000e+04   2.83839502e-01  -1.79271796e-01\n",
        "   -1.27200000e+03]]\n",
        "(4400, 5)\n",
        "[[ 0.45149742]\n",
        " [ 0.45981443]\n",
        " [ 0.88020729]\n",
        " ..., \n",
        " [ 0.5091633 ]\n",
        " [ 0.71292758]\n",
        " [ 0.61165018]]\n",
        "AuC score on CV data: 0.795092706555\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}