{
 "metadata": {
  "name": "",
  "signature": "sha256:43c5bbc19981a1df4f40ae1fd24be2adfb2430a76fd8665d4615776d0c369f72"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import absolute_import\n",
      "from __future__ import print_function\n",
      "from functools import reduce\n",
      "import re\n",
      "import tarfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.random.seed(1337)  # for reproducibility"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.datasets.data_utils import get_file\n",
      "from keras.layers.embeddings import Embedding\n",
      "from keras.layers.core import Dense, Merge\n",
      "from keras.layers import recurrent\n",
      "from keras.models import Sequential\n",
      "from keras.preprocessing.sequence import pad_sequences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize(sent):\n",
      "    '''Return the tokens of a sentence including punctuation.\n",
      "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
      "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
      "    '''\n",
      "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
      "\n",
      "\n",
      "def parse_stories(lines, only_supporting=False):\n",
      "    '''Parse stories provided in the bAbi tasks format\n",
      "    If only_supporting is true, only the sentences that support the answer are kept.\n",
      "    '''\n",
      "    data = []\n",
      "    story = []\n",
      "    for line in lines:\n",
      "        line = line.decode('utf-8').strip()\n",
      "        nid, line = line.split(' ', 1)\n",
      "        nid = int(nid)\n",
      "        if nid == 1:\n",
      "            story = []\n",
      "        if '\\t' in line:\n",
      "            q, a, supporting = line.split('\\t')\n",
      "            q = tokenize(q)\n",
      "            substory = None\n",
      "            if only_supporting:\n",
      "                # Only select the related substory\n",
      "                supporting = map(int, supporting.split())\n",
      "                substory = [story[i - 1] for i in supporting]\n",
      "            else:\n",
      "                # Provide all the substories\n",
      "                substory = [x for x in story if x]\n",
      "            data.append((substory, q, a))\n",
      "            story.append('')\n",
      "        else:\n",
      "            sent = tokenize(line)\n",
      "            story.append(sent)\n",
      "    return data\n",
      "\n",
      "\n",
      "def get_stories(f, only_supporting=False, max_length=None):\n",
      "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
      "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
      "    '''\n",
      "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
      "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
      "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
      "    return data\n",
      "\n",
      "\n",
      "def vectorize_stories(data):\n",
      "    X = []\n",
      "    Xq = []\n",
      "    Y = []\n",
      "    for story, query, answer in data:\n",
      "        x = [word_idx[w] for w in story]\n",
      "        xq = [word_idx[w] for w in query]\n",
      "        y = np.zeros(vocab_size)\n",
      "        y[word_idx[answer]] = 1\n",
      "        X.append(x)\n",
      "        Xq.append(xq)\n",
      "        Y.append(y)\n",
      "    return pad_sequences(X, maxlen=story_maxlen), pad_sequences(Xq, maxlen=query_maxlen), np.array(Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RNN = recurrent.GRU\n",
      "EMBED_HIDDEN_SIZE = 50\n",
      "SENT_HIDDEN_SIZE = 100\n",
      "QUERY_HIDDEN_SIZE = 100\n",
      "BATCH_SIZE = 32\n",
      "EPOCHS = 20\n",
      "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.GRU'>, 50, 100, 100\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = get_file('babi-tasks-v1-2.tar.gz', origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n",
      "tar = tarfile.open(path)\n",
      "# Default QA1 with 1000 samples\n",
      "# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n",
      "# QA1 with 10,000 samples\n",
      "# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n",
      "# QA2 with 1000 samples\n",
      "challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n",
      "# QA2 with 10,000 samples\n",
      "# challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
      "train = get_stories(tar.extractfile(challenge.format('train')))\n",
      "test = get_stories(tar.extractfile(challenge.format('test')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n",
      "# Reserve 0 for masking via pad_sequences\n",
      "vocab_size = len(vocab) + 1\n",
      "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
      "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
      "query_maxlen = max(map(len, (x for _, x, _ in train + test)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}